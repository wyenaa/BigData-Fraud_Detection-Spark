{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "948ac4dd-48ad-4326-a428-dfe4be4b1c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "python_path=\"/home/hduser/miniconda3/envs/pyspark/bin/python\"\n",
    "\n",
    "os.environ['PYSPARK_PYTHON']= python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON']=python_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc4985d7-d9e0-4117-9f5c-cb8d0f19f491",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/01 06:36:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "26/01/01 06:36:07 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 4.1.0\n",
      "Spark master: yarn\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try: \n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "    \n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Fraud_ETL_YARN_Job\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs:/hadoop-master:9000/user/hive/warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"Spark master:\", spark.sparkContext.master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3275a825-fd67-4cc2-8e72-1bf0ac36e01e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+--------------+----------------+-------+----------------+-----------------+--------+-----------+--------+----------+---------------------------+------------------------+--------------+-----------------+---------------+---------------+-----------+\n",
      "|transaction_id|           timestamp|sender_account|receiver_account| amount|transaction_type|merchant_category|location|device_used|is_fraud|fraud_type|time_since_last_transaction|spending_deviation_score|velocity_score|geo_anomaly_score|payment_channel|     ip_address|device_hash|\n",
      "+--------------+--------------------+--------------+----------------+-------+----------------+-----------------+--------+-----------+--------+----------+---------------------------+------------------------+--------------+-----------------+---------------+---------------+-----------+\n",
      "|       T100000|2023-08-22 09:22:...|     ACC877572|       ACC388389| 343.78|      withdrawal|        utilities|   Tokyo|     mobile|   false|      NULL|                       NULL|                   -0.21|             3|             0.22|           card| 13.101.214.112|   D8536477|\n",
      "|       T100001|2023-08-04 01:58:...|     ACC895667|       ACC944962| 419.65|      withdrawal|           online| Toronto|        atm|   false|      NULL|                       NULL|                   -0.14|             7|             0.96|            ACH|  172.52.47.194|   D2622631|\n",
      "|       T100002|2023-05-12 11:39:...|     ACC733052|       ACC377370|2773.86|         deposit|            other|  London|        pos|   false|      NULL|                       NULL|                   -1.78|            20|             0.89|           card|   185.98.35.23|   D4823498|\n",
      "|       T100003|2023-10-10 06:04:...|     ACC996865|       ACC344098|1666.22|         deposit|           online|  Sydney|        pos|   false|      NULL|                       NULL|                    -0.6|             6|             0.37|  wire_transfer|  107.136.36.87|   D9961380|\n",
      "|       T100004|2023-09-24 08:09:...|     ACC584714|       ACC497887|  24.43|        transfer|        utilities| Toronto|     mobile|   false|      NULL|                       NULL|                    0.79|            13|             0.27|            ACH|108.161.108.255|   D7637601|\n",
      "+--------------+--------------------+--------------+----------------+-------+----------------+-----------------+--------+-----------+--------+----------+---------------------------+------------------------+--------------+-----------------+---------------+---------------+-----------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "hdfs_input_path = \"hdfs://hadoop-master:9000/hduser/project/fraud_detection/data/Dataset_Fraud_Detection/financial_fraud_detection_dataset.csv\"\n",
    "df = spark.read.csv(hdfs_input_path, header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda469d0-5879-47a4-9d7a-e655631cc055",
   "metadata": {},
   "source": [
    "# *Exploratory Data Analysis (EDA)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa2368f7-bb88-47d8-96a2-229327575244",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct, count, when, isnan, lit, col, avg, stddev, min, max, corr\n",
    "from pyspark.sql.types import DoubleType, FloatType, IntegerType, LongType\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a28482b2-0d9a-44f6-8e22-ef2120f861f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- transaction_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- sender_account: string (nullable = true)\n",
      " |-- receiver_account: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- merchant_category: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- device_used: string (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      " |-- fraud_type: string (nullable = true)\n",
      " |-- time_since_last_transaction: double (nullable = true)\n",
      " |-- spending_deviation_score: double (nullable = true)\n",
      " |-- velocity_score: integer (nullable = true)\n",
      " |-- geo_anomaly_score: double (nullable = true)\n",
      " |-- payment_channel: string (nullable = true)\n",
      " |-- ip_address: string (nullable = true)\n",
      " |-- device_hash: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb04414a-1582-4d34-a11e-a900f181afe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total baris data:  {5000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "total_rows = df.count()\n",
    "print(\"Total baris data: \",  {total_rows})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c024c-80dc-4728-be4a-ebec3ab6303d",
   "metadata": {},
   "source": [
    "# Missing Value Checking and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8507d46-f4c0-42e0-a4f0-d31e7c2c870f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------+\n",
      "|column                     |missing_count|\n",
      "+---------------------------+-------------+\n",
      "|transaction_id             |0            |\n",
      "|timestamp                  |0            |\n",
      "|sender_account             |0            |\n",
      "|receiver_account           |0            |\n",
      "|amount                     |0            |\n",
      "|transaction_type           |0            |\n",
      "|merchant_category          |0            |\n",
      "|location                   |0            |\n",
      "|device_used                |0            |\n",
      "|is_fraud                   |0            |\n",
      "|fraud_type                 |4820447      |\n",
      "|time_since_last_transaction|896513       |\n",
      "|spending_deviation_score   |0            |\n",
      "|velocity_score             |0            |\n",
      "|geo_anomaly_score          |0            |\n",
      "|payment_channel            |0            |\n",
      "|ip_address                 |0            |\n",
      "|device_hash                |0            |\n",
      "+---------------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    c = field.name\n",
    "    dt = field.dataType\n",
    "\n",
    "    if isinstance(dt, (DoubleType, FloatType)):\n",
    "        missing = df.select(\n",
    "            count(when(isnan(col(c)) | col(c).isNull(), c)).alias(\"missing\")\n",
    "        ).collect()[0][\"missing\"]\n",
    "    else:\n",
    "        missing = df.select(\n",
    "            count(when(col(c).isNull(), c)).alias(\"missing\")\n",
    "        ).collect()[0][\"missing\"]\n",
    "\n",
    "    rows.append((c, missing))\n",
    "\n",
    "result_df = spark.createDataFrame(rows, [\"column\", \"missing_count\"])\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e185a0d-4a12-4e20-8f76-20020a69f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = [\n",
    "    'transaction_id',\n",
    "    'timestamp',\n",
    "    'sender_account',\n",
    "    'receiver_account',\n",
    "    'fraud_type',\n",
    "    'device_hash',\n",
    "    'ip_address',\n",
    "    'location'\n",
    "]\n",
    "df_clean = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8005a8ee-e83b-424f-983a-95c0edc412a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled = df_clean.fillna({'time_since_last_transaction': -1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a18bb3ec-27e8-476f-bbce-4b26c0951336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris saat ini: {5000000}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 95:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------+\n",
      "|column                     |missing_count|\n",
      "+---------------------------+-------------+\n",
      "|amount                     |0            |\n",
      "|transaction_type           |0            |\n",
      "|merchant_category          |0            |\n",
      "|device_used                |0            |\n",
      "|is_fraud                   |0            |\n",
      "|time_since_last_transaction|0            |\n",
      "|spending_deviation_score   |0            |\n",
      "|velocity_score             |0            |\n",
      "|geo_anomaly_score          |0            |\n",
      "|payment_channel            |0            |\n",
      "+---------------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Jumlah baris saat ini:\", {df_filled.count()})\n",
    "\n",
    "rows = []\n",
    "\n",
    "for field in df_filled.schema.fields:\n",
    "    c = field.name\n",
    "    dt = field.dataType\n",
    "\n",
    "    if isinstance(dt, (DoubleType, FloatType)):\n",
    "        missing = df_filled.select(\n",
    "            count(when(isnan(col(c)) | col(c).isNull(), c)).alias(\"missing\")\n",
    "        ).collect()[0][\"missing\"]\n",
    "    else:\n",
    "        # STRING, BOOLEAN, TIMESTAMP, INTEGER, dll\n",
    "        missing = df_filled.select(\n",
    "            count(when(col(c).isNull(), c)).alias(\"missing\")\n",
    "        ).collect()[0][\"missing\"]\n",
    "\n",
    "    rows.append((c, missing))\n",
    "\n",
    "missing_df = spark.createDataFrame(rows, [\"column\", \"missing_count\"])\n",
    "missing_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62256294-9771-4724-b44c-757a4fa06ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 97:======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+\n",
      "|is_fraud|  count|\n",
      "+--------+-------+\n",
      "|    true| 179553|\n",
      "|   false|4820447|\n",
      "+--------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"is_fraud\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6185703d-d7f3-4073-b47f-b893c45429c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 108:===============================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+-----+\n",
      "|transaction_type|  false| true|\n",
      "+----------------+-------+-----+\n",
      "|      withdrawal|1203761|44874|\n",
      "|         deposit|1205807|44786|\n",
      "|        transfer|1205006|45328|\n",
      "|         payment|1205873|44565|\n",
      "+----------------+-------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"transaction_type\").pivot(\"is_fraud\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f50a7e-398b-42c5-8da9-f52ac1f0236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 114:===============================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|transaction_type|  count|\n",
      "+----------------+-------+\n",
      "|         deposit|1250593|\n",
      "|         payment|1250438|\n",
      "|        transfer|1250334|\n",
      "|      withdrawal|1248635|\n",
      "+----------------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"transaction_type\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8794d8e8-1c35-49a5-bab0-ff7dac1cc76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- amount: double (nullable = true)\n",
      " |-- transaction_type: string (nullable = true)\n",
      " |-- merchant_category: string (nullable = true)\n",
      " |-- device_used: string (nullable = true)\n",
      " |-- is_fraud: boolean (nullable = true)\n",
      " |-- time_since_last_transaction: double (nullable = false)\n",
      " |-- spending_deviation_score: double (nullable = true)\n",
      " |-- velocity_score: integer (nullable = true)\n",
      " |-- geo_anomaly_score: double (nullable = true)\n",
      " |-- payment_channel: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_filled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b33180b-fe05-423f-a2a8-dbe0182bdb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 117:======================================>                  (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|merchant_category| count|\n",
      "+-----------------+------+\n",
      "|           retail|626319|\n",
      "|           travel|625656|\n",
      "|       restaurant|625483|\n",
      "|    entertainment|625332|\n",
      "|          grocery|624954|\n",
      "|            other|624589|\n",
      "|        utilities|624086|\n",
      "|           online|623581|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"merchant_category\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78453b3e-523b-4cdc-99b6-b29084edf1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 120:===============================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|device_used|  count|\n",
      "+-----------+-------+\n",
      "|     mobile|1251131|\n",
      "|        web|1250071|\n",
      "|        atm|1249640|\n",
      "|        pos|1249158|\n",
      "+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"device_used\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503c37f6-ffe7-4932-925b-c63e92e0269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [\n",
    "    'amount',\n",
    "    'time_since_last_transaction',\n",
    "    'spending_deviation_score',\n",
    "    'velocity_score',\n",
    "    'geo_anomaly_score'\n",
    "]\n",
    "df_filled = df_filled.withColumn(\"is_fraud\", col (\"is_fraud\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4f7555f-485b-4fdb-9dbc-16f26aa94b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 04:02:41 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 125:>                                                        (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------------------------+------------------------+-----------------+-------------------+\n",
      "|summary|            amount|time_since_last_transaction|spending_deviation_score|   velocity_score|  geo_anomaly_score|\n",
      "+-------+------------------+---------------------------+------------------------+-----------------+-------------------+\n",
      "|   mean|  358.934268763911|         1.0729165780591507|    -3.88115999999996...|       10.5013196| 0.5000292560000075|\n",
      "| stddev|469.93331106593666|         3240.0977344690036|      1.0008069793965708|5.766842438453669|0.28863493504097754|\n",
      "|    min|              0.01|         -8777.814181944444|                   -5.26|                1|                0.0|\n",
      "|    25%|             26.57|        -1920.5026717130556|                   -0.68|                6|               0.25|\n",
      "|    50%|            138.66|                       -1.0|                     0.0|               11|                0.5|\n",
      "|    75%|            503.84|         1923.9303089991668|                    0.67|               16|               0.75|\n",
      "|    max|           3520.57|          8757.758483437501|                    5.02|               20|                1.0|\n",
      "+-------+------------------+---------------------------+------------------------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_filled.select(numeric_cols).summary(\"mean\", \"stddev\", \"min\", \"25%\", \"50%\", \"75%\", \"max\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0db65a6-7c1d-4885-8c4b-86043f23e62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 126:======================================>                  (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|is_fraud|       avg_amount|  avg_spending_score|      avg_velocity|     avg_geo_score|    avg_time_since|\n",
      "+--------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "|    true|358.5281994731318|6.518409606077314E-4|10.512377960824937|0.5004939488618937|1.7653716579192553|\n",
      "|   false|358.9493941162692|-4.26852530481104...| 10.50090769590455| 0.500011947024837| 1.514836414934929|\n",
      "+--------+-----------------+--------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.groupBy(\"is_fraud\").agg(\n",
    "    avg(\"amount\").alias(\"avg_amount\"),\n",
    "    avg(\"spending_deviation_score\").alias(\"avg_spending_score\"),\n",
    "    avg(\"velocity_score\").alias(\"avg_velocity\"),\n",
    "    avg(\"geo_anomaly_score\").alias(\"avg_geo_score\"),\n",
    "    avg(\"time_since_last_transaction\").alias(\"avg_time_since\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09dfd315-252b-41f5-8410-fd641e64fdca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai mendekati 1 = Berhubungan kuat (positif)\n",
      "Nilai mendekati -1 = Berhubungan kuat (nehatif/kebalikan)\n",
      "Nilai 0 = Tidak ada kaitan sama sekali\n",
      "                               amount  time_since_last_transaction  \\\n",
      "amount                       1.000000                     0.000286   \n",
      "time_since_last_transaction  0.000286                     1.000000   \n",
      "spending_deviation_score     0.000799                     0.000182   \n",
      "velocity_score              -0.000811                     0.000080   \n",
      "geo_anomaly_score            0.000176                    -0.000028   \n",
      "\n",
      "                             spending_deviation_score  velocity_score  \\\n",
      "amount                                       0.000799       -0.000811   \n",
      "time_since_last_transaction                  0.000182        0.000080   \n",
      "spending_deviation_score                     1.000000       -0.000155   \n",
      "velocity_score                              -0.000155        1.000000   \n",
      "geo_anomaly_score                            0.000472        0.000340   \n",
      "\n",
      "                             geo_anomaly_score  \n",
      "amount                                0.000176  \n",
      "time_since_last_transaction          -0.000028  \n",
      "spending_deviation_score              0.000472  \n",
      "velocity_score                        0.000340  \n",
      "geo_anomaly_score                     1.000000  \n"
     ]
    }
   ],
   "source": [
    "assembler = VectorAssembler(inputCols=numeric_cols, outputCol=\"features_corr\")\n",
    "df_vector = assembler.transform(df_filled).select(\"features_corr\")\n",
    "\n",
    "matrix = Correlation.corr(df_vector, \"features_corr\").head()\n",
    "corr_array = matrix[0].toArray()\n",
    "\n",
    "pdf_corr = pd.DataFrame(corr_array, columns=numeric_cols, index=numeric_cols)\n",
    "print(\"Nilai mendekati 1 = Berhubungan kuat (positif)\")\n",
    "print(\"Nilai mendekati -1 = Berhubungan kuat (nehatif/kebalikan)\")\n",
    "print(\"Nilai 0 = Tidak ada kaitan sama sekali\")\n",
    "print(pdf_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb8aea7a-2965-4105-90d0-ea07149e509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolerasi Fitur Terhadap Target (is_fraud)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount : -0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_since_last_transaction : 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spending_deviation_score : 0.0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "velocity_score : 0.0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 149:======================================>                  (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geo_anomaly_score : 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "print(\"Kolerasi Fitur Terhadap Target (is_fraud)\")\n",
    "for c in numeric_cols:\n",
    "    val = df_filled.stat.corr(c, \"is_fraud\")\n",
    "    print(f\"{c} : {val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad7b23a5-fee2-4bed-a5e9-4f1aab94524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "cat_cols = ['transaction_type', 'merchant_category', 'device_used', 'payment_channel']\n",
    "stages = []\n",
    "\n",
    "for col_name in cat_cols:\n",
    "    indexer = StringIndexer(inputCol=col_name, outputCol=col_name + \"_index\")\n",
    "    stages.append(indexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86af9d1a-1c9c-4dda-9253-3e2c302be71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitur final: ['amount', 'time_since_last_transaction', 'spending_deviation_score', 'velocity_score', 'geo_anomaly_score', 'transaction_type_index', 'merchant_category_index', 'device_used_index', 'payment_channel_index']\n"
     ]
    }
   ],
   "source": [
    "input_cols = numeric_cols + [c + \"_index\" for c in cat_cols]\n",
    "\n",
    "print(f\"fitur final: {input_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be98c4f9-707b-4d33-9b10-db7b6c4e81b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "stages=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79d9c63d-9c7d-446a-9dee-2955ecc4ccca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=c,\n",
    "        outputCol=c + \"_index\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCols=[c + \"_index\"],\n",
    "        outputCols=[c + \"_ohe\"]\n",
    "    )\n",
    "    stages += [indexer, encoder]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "153ccacb-d479-4846-bc4f-e0dccea0db5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = numeric_cols + [c + \"_ohe\" for c in cat_cols]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=input_cols,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "stages.append(assembler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "833efce5-ccda-4bee-a125-042f4585e62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=stages)\n",
    "model_pipeline = pipeline.fit(df_filled)\n",
    "df_final = model_pipeline.transform(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d8b2bd38-f433-46d8-9367-59257713034b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+--------+\n",
      "|features                                                                  |is_fraud|\n",
      "+--------------------------------------------------------------------------+--------+\n",
      "|(25,[0,1,2,3,4,8,15,17,23],[343.78,-1.0,-0.21,3.0,0.22,1.0,1.0,1.0,1.0])  |0       |\n",
      "|(25,[0,1,2,3,4,8,16,19,22],[419.65,-1.0,-0.14,7.0,0.96,1.0,1.0,1.0,1.0])  |0       |\n",
      "|(25,[0,1,2,3,4,5,14,20,23],[2773.86,-1.0,-1.78,20.0,0.89,1.0,1.0,1.0,1.0])|0       |\n",
      "|(25,[0,1,2,3,4,5,16,20,21],[1666.22,-1.0,-0.6,6.0,0.37,1.0,1.0,1.0,1.0])  |0       |\n",
      "|(25,[0,1,2,3,4,7,15,17,22],[24.43,-1.0,0.79,13.0,0.27,1.0,1.0,1.0,1.0])   |0       |\n",
      "+--------------------------------------------------------------------------+--------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "data_ready = df_final.select(\"features\", \"is_fraud\")\n",
    "data_ready.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b2cbb55d-8118-473a-93e3-c7b25ad265ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "data_ready.write.mode(\"overwrite\").parquet(\n",
    "    \"hdfs://hadoop-master:9000/hduser/project/fraud_detection/data/ready_for_training\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc2efbf-90ed-42d7-8c7a-c9da489cf8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 04:07:33 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Fraud_Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1df59f6-4ed9-4224-9dec-16ff86c0c076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import IntegerType, DoubleType\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b6dc11eb-8b06-43cb-902e-15522cbe7656",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpakSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSpakSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFraud_Project\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SpakSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SpakSession.builder.appName(\"Fraud_Project\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28a96074-8954-4cb9-be9a-18165d819e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data bersih: 5000000 baris\n"
     ]
    }
   ],
   "source": [
    "path = \"hdfs://hadoop-master:9000/hduser/project/fraud_detection/data/ready_for_training\"\n",
    "df = spark.read.parquet(path)\n",
    "\n",
    "print(f\"Total data bersih: {df.count()} baris\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c6acfe-8be0-4b94-b7b1-9e6887b2d325",
   "metadata": {},
   "source": [
    "## Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d22a7b25-0310-4618-9350-1249ff79c34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data training: 3999208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 185:============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah data test: 1000792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Jumlah data training: {train_data.count()}\")\n",
    "print(f\"Jumlah data test: {test_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c7343-6ef1-4496-9baa-4556077a1b90",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ce39abe-9d73-4667-9659-7d1cdd5a1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol=\"is_fraud\", featuresCol=\"features\", numTrees=50, maxDepth=10, maxBins=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "63c479e9-c0ac-45e2-b249-b6343baba1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 04:15:44 WARN DAGScheduler: Broadcasting large task binary with size 1858.2 KiB\n",
      "26/01/01 04:16:45 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "26/01/01 04:17:52 WARN DAGScheduler: Broadcasting large task binary with size 6.0 MiB\n",
      "26/01/01 04:19:10 WARN DAGScheduler: Broadcasting large task binary with size 1461.1 KiB\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "model = rf.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b15e3788-fa7a-4dc7-a90f-577c1cf7b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7b0f1c-97ec-4d17-b5b0-7c025ab26e74",
   "metadata": {},
   "source": [
    "## Evaluasi Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5bff000a-b7f2-4e67-b634-7540686ded4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 04:52:02 WARN DAGScheduler: Broadcasting large task binary with size 1400.0 KiB\n",
      "[Stage 244:>                                                        (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score: 59.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "auc_evaluation = BinaryClassificationEvaluator(labelCol=\"is_fraud\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "\n",
    "auc = auc_evaluation.evaluate(predictions)\n",
    "print(f\"AUC Score: {auc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cd440632-6dfb-4891-8f60-ebbc54219c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Evaluasi Model\n",
      "Accuracy: 96.40%\n",
      "F1 Score: 94.64%\n",
      "AUC Score: 59.16%\n"
     ]
    }
   ],
   "source": [
    "print(\"Hasil Evaluasi Model\")\n",
    "print(f\"Accuracy: {accuracy*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")\n",
    "print(f\"AUC Score: {auc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16ac49c8-92d4-4617-a844-a82fb6cb340c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 04:58:28 WARN DAGScheduler: Broadcasting large task binary with size 1407.9 KiB\n",
      "26/01/01 04:59:10 WARN DAGScheduler: Broadcasting large task binary with size 1407.9 KiB\n",
      "26/01/01 04:59:59 WARN DAGScheduler: Broadcasting large task binary with size 1407.9 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9294\n",
      "Recall: 0.9640\n",
      "F1 Score: 0.9464\n",
      "\n",
      "Confusion Matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 05:01:18 WARN DAGScheduler: Broadcasting large task binary with size 1401.3 KiB\n",
      "26/01/01 05:03:13 WARN DAGScheduler: Broadcasting large task binary with size 1391.8 KiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|is_fraud|prediction| count|\n",
      "+--------+----------+------+\n",
      "|       1|       0.0| 35988|\n",
      "|       0|       0.0|964804|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/01 05:46:36 ERROR TransportClient: Failed to send RPC RPC 7949047576278228036 to /192.168.149.129:53382\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:37 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 2 at RPC address 192.168.149.129:44614, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 7949047576278228036 to /192.168.149.129:53382: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:132)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:870)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:731)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1386)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:823)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1136)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:148)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:141)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:535)\n",
      "\tat io.netty.channel.SingleThreadIoEventLoop.run(SingleThreadIoEventLoop.java:201)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:1193)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:37 ERROR Client: Failed to contact YARN for application application_1767214300327_0001.\n",
      "java.net.ConnectException: Call From hadoop-slave1/192.168.149.129 to hadoop-master:8032 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n",
      "\tat java.base/jdk.internal.reflect.GeneratedConstructorAccessor83.newInstance(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:961)\n",
      "\tat org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:876)\n",
      "\tat org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1588)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1529)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1426)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:258)\n",
      "\tat org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:139)\n",
      "\tat jdk.proxy2/jdk.proxy2.$Proxy29.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getApplicationReport(ApplicationClientProtocolPBClientImpl.java:256)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:437)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:170)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:162)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:100)\n",
      "\tat org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:366)\n",
      "\tat jdk.proxy2/jdk.proxy2.$Proxy30.getApplicationReport(Unknown Source)\n",
      "\tat org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport(YarnClientImpl.java:600)\n",
      "\tat org.apache.spark.deploy.yarn.Client.getApplicationReport(Client.scala:394)\n",
      "\tat org.apache.spark.deploy.yarn.Client.monitorApplication(Client.scala:1210)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:119)\n",
      "Caused by: java.net.ConnectException: Connection refused\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n",
      "\tat java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)\n",
      "\tat org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)\n",
      "\tat org.apache.hadoop.net.NetUtils.connect(NetUtils.java:614)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:668)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:789)\n",
      "\tat org.apache.hadoop.ipc.Client$Connection.access$3800(Client.java:364)\n",
      "\tat org.apache.hadoop.ipc.Client.getConnection(Client.java:1649)\n",
      "\tat org.apache.hadoop.ipc.Client.call(Client.java:1473)\n",
      "\t... 18 more\n",
      "26/01/01 05:46:37 ERROR YarnClientSchedulerBackend: YARN application has exited unexpectedly with state FAILED! Check the YARN application logs for more details.\n",
      "26/01/01 05:46:37 ERROR TransportClient: Failed to send RPC RPC 5723227013352824812 to /192.168.149.129:53382\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:37 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to get executor loss reason for executor id 1 at RPC address 192.168.149.129:44598, but got no response. Marking as agent lost.\n",
      "java.io.IOException: Failed to send RPC RPC 5723227013352824812 to /192.168.149.129:53382: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:506)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setValue0(DefaultPromise.java:650)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.setFailure0(DefaultPromise.java:643)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.tryFailure(DefaultPromise.java:132)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetFailure(AbstractChannel.java:870)\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(AbstractChannel.java:731)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.write(DefaultChannelPipeline.java:1386)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:823)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext$WriteTask.run(AbstractChannelHandlerContext.java:1136)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:148)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:141)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:535)\n",
      "\tat io.netty.channel.SingleThreadIoEventLoop.run(SingleThreadIoEventLoop.java:201)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:1193)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:37 ERROR YarnClientSchedulerBackend: Diagnostics message: Failed to contact YARN for application application_1767214300327_0001.\n",
      "26/01/01 05:46:37 ERROR YarnScheduler: Lost executor 2 on hadoop-slave1: Executor Process Lost\n",
      "26/01/01 05:46:37 ERROR YarnScheduler: Lost executor 1 on hadoop-slave1: Executor Process Lost\n",
      "26/01/01 05:46:39 ERROR TransportClient: Failed to send RPC RPC 8371171860649252308 to /192.168.149.129:53382\n",
      "io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:39 ERROR YarnSchedulerBackend$YarnSchedulerEndpoint: Sending RequestExecutors(Map(),Map(),Map(),Set()) to AM was unsuccessful\n",
      "java.io.IOException: Failed to send RPC RPC 8371171860649252308 to /192.168.149.129:53382: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:37)\n",
      "\tat io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:517)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:148)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:141)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:535)\n",
      "\tat io.netty.channel.SingleThreadIoEventLoop.run(SingleThreadIoEventLoop.java:201)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:1193)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n",
      "26/01/01 05:46:39 ERROR Utils: Uncaught exception in thread YARN application state monitor\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.requestTotalExecutors(CoarseGrainedSchedulerBackend.scala:871)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnSchedulerBackend.stop(YarnSchedulerBackend.scala:115)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:179)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$stop$2(TaskSchedulerImpl.scala:933)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1314)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:933)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$4(DAGScheduler.scala:3191)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1314)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:3191)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2357)\n",
      "\tat org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1314)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2357)\n",
      "\tat org.apache.spark.SparkContext.stop(SparkContext.scala:2308)\n",
      "\tat org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$MonitorThread.run(YarnClientSchedulerBackend.scala:126)\n",
      "Caused by: java.io.IOException: Failed to send RPC RPC 8371171860649252308 to /192.168.149.129:53382: io.netty.channel.StacklessClosedChannelException\n",
      "\tat org.apache.spark.network.client.TransportClient$RpcChannelListener.handleFailure(TransportClient.java:395)\n",
      "\tat org.apache.spark.network.client.TransportClient$StdChannelListener.operationComplete(TransportClient.java:372)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:604)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:571)\n",
      "\tat io.netty.util.concurrent.DefaultPromise.access$200(DefaultPromise.java:37)\n",
      "\tat io.netty.util.concurrent.DefaultPromise$1.run(DefaultPromise.java:517)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.runTask(AbstractEventExecutor.java:148)\n",
      "\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:141)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:535)\n",
      "\tat io.netty.channel.SingleThreadIoEventLoop.run(SingleThreadIoEventLoop.java:201)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:1193)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: io.netty.channel.StacklessClosedChannelException\n",
      "\tat io.netty.channel.AbstractChannel$AbstractUnsafe.write(Object, ChannelPromise)(Unknown Source)\n"
     ]
    }
   ],
   "source": [
    "multi_evaluator = MulticlassClassificationEvaluator(labelCol=\"is_fraud\", predictionCol=\"prediction\")\n",
    "\n",
    "precision = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"weightedRecall\"})\n",
    "f1 =  multi_evaluator.evaluate(predictions, {multi_evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion Matrix\")\n",
    "predictions.groupBy(\"is_fraud\", \"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15485f-81d5-4639-ab44-97a2af90e132",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PySpark)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
